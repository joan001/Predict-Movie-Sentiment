{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict_Movie_Sentiment",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj4bW9VP1Qv1"
      },
      "source": [
        "# Use 1D Convolutional Neural Network for text mining in IMDB( Internet Movie Database) to predicate sentimental analysis "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHqD8UQd2BKm"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xrKgHS12MEf"
      },
      "source": [
        "# Keras has this dataset built in, we can just load it. Also, the words have been replaced by integers  in the dataset\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucyk_ERK2El-"
      },
      "source": [
        "# Join the dataset to see the shape of dataset\r\n",
        "X = np.concatenate((X_train, X_test), axis=0)\r\n",
        "y = np.concatenate((y_train, y_test), axis=0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvg8vvRo3NFv",
        "outputId": "025ac556-001e-4308-9568-8f1abda2d6a9"
      },
      "source": [
        "#print the shape of dataset. we can see the total is 50,000 rows\r\n",
        "print(\"The shape of the dataset is: \" )\r\n",
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is: \n",
            "(50000,)\n",
            "(50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNtZL9AJ4G_k",
        "outputId": "0f7ea811-c727-4f9a-b5a0-eff7c86e464c"
      },
      "source": [
        "# Check unique class value. We can see that it is a binary classification problem for good and bad sentiment in the movie review\r\n",
        "print(\"Class:\")\r\n",
        "print(np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class:\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypi8UfNP49iJ",
        "outputId": "e3eadb35-03cf-40f6-9fa0-a86f32c458fc"
      },
      "source": [
        "# Check the the total number of unique words in the dataset. We can see that there are less 100,000 words used in whole dataset,\r\n",
        "print(len(np.unique(np.hstack(X))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOZU7gCx6Gfg"
      },
      "source": [
        "# Let's we are interested in only the first 10,000 word in dataset. So we can load dataset as below\r\n",
        "top_words = 10000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3lUoauE7jRJ",
        "outputId": "b95ed385-4163-4223-f6fd-c5057b579f85"
      },
      "source": [
        "#  we can get the average of the reveiw length and standard deviation, we can see the review length is below 500\r\n",
        "review_len = [len(x) for x in X]\r\n",
        "print(\"the mean and standard deviation are\")\r\n",
        "print(np.mean(review_len), np.std(review_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the mean and standard deviation are\n",
            "234.75892 172.91149458735703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chT8lny_9BR3"
      },
      "source": [
        "# We'll choose review length as 500 and trancate if longer that and pad with 0 if short than that\r\n",
        "max_words = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo8_6whW-F84"
      },
      "source": [
        "#Define model\r\n",
        "model = Sequential()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vxSkjq-7L8"
      },
      "source": [
        "# Add word embedding layer with 32- dimension vector to represent each word\r\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nk-LDYl_OnW"
      },
      "source": [
        "# Addd 1D CNN layer\r\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCaO3UYUAbPM"
      },
      "source": [
        "# Add default Maxpooling\r\n",
        "model.add(MaxPooling1D())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snb6TBIXAvMJ"
      },
      "source": [
        "# Add flatten  layer\r\n",
        "model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVuyahfA2lH"
      },
      "source": [
        "# Add densen layer \r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlhhwc1yBMwO"
      },
      "source": [
        "#Compile the model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWdpDbrHBU9Z",
        "outputId": "edd6156b-1781-4c5c-b752-143ec25ea7b2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 500, 32)           320000    \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 250)               2000250   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,323,605\n",
            "Trainable params: 2,323,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqIM4lCiBZFf",
        "outputId": "79434510-d9c2-42ba-c4b7-8bc7eca237a6"
      },
      "source": [
        "# Fit the model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 26s - loss: 0.4678 - accuracy: 0.7338 - val_loss: 0.2818 - val_accuracy: 0.8806\n",
            "Epoch 2/2\n",
            "196/196 - 26s - loss: 0.1946 - accuracy: 0.9266 - val_loss: 0.2785 - val_accuracy: 0.8861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe4e6a179e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8El8fvBdn0",
        "outputId": "2b4fe3c1-fb5c-4707-85f4-4ff9b37e9fc9"
      },
      "source": [
        "# Final evaluation of the model.  We can see that the model achieves the accuracy of 88.61% \r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(scores[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 88.61%\n",
            "0.27853327989578247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu9ZV4SaF-PK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}